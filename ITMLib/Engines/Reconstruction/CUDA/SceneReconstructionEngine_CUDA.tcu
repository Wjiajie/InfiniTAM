// Copyright 2014-2017 Oxford University Innovation Limited and the authors of InfiniTAM

#include "SceneReconstructionEngine_CUDA.h"

#include "../Shared/SceneReconstructionEngine_Shared.h"
#include "../../../Utils/CUDAUtils.h"
#include "../../../../ORUtils/JetbrainsCUDASyntax.hpp"
#include "../../Common/CheckBlockVisibility.h"
#include "../../Common/AllocationTempData.h"
#include "../../Indexing/Shared/IndexingEngine_Shared.h"
#include "../../../Utils/Configuration.h"


using namespace ITMLib;

namespace {

template<class TVoxel, bool stopMaxW>
__global__ void integrateIntoScene_device(TVoxel* localVBA, const ITMHashEntry* hashTable, int* noVisibleEntryIDs,
                                          const Vector4u* rgb, Vector2i rgbImgSize, const float* depth,
                                          const float* confidence, Vector2i imgSize, Matrix4f M_d, Matrix4f M_rgb,
                                          Vector4f projParams_d, Vector4f projParams_rgb, float _voxelSize, float mu,
                                          int maxW);

template<class TVoxel, bool stopMaxW>
__global__ void integrateIntoScene_device(TVoxel* voxelArray, const PlainVoxelArray::GridAlignedBox* arrayInfo,
                                          const Vector4u* rgb, Vector2i rgbImgSize, const float* depth,
                                          const float* confidence, Vector2i depthImgSize, Matrix4f M_d, Matrix4f M_rgb,
                                          Vector4f projParams_d, Vector4f projParams_rgb, float _voxelSize, float mu,
                                          int maxW);

__global__ void buildHashAllocAndVisibleType_device(
		ITMLib::HashEntryAllocationState* hashEntryStates, HashBlockVisibility* blockVisibilityTypes,
		Vector3s* blockCoords, const float* depth, Matrix4f invM_d, Vector4f projParams_d, float surface_cutoff_distance, Vector2i _imgSize,
		float oneOverHashBlockSize_Meters, ITMHashEntry* hashTable, float near_clipping_distance, float far_clipping_distance);

__global__ void allocateVoxelBlocksList_device(int* voxelAllocationList, int* excessAllocationList,
                                               ITMHashEntry* hashTable, const int hashEntryCount,
                                               AllocationTempData* allocData, HashEntryAllocationState* hashEntryStates,
                                               HashBlockVisibility* blockVisibilityTypes, Vector3s* blockCoordinates);

__global__ void
reAllocateSwappedOutVoxelBlocks_device(int* voxelAllocationList, ITMHashEntry* hashTable, int hashEntryCount,
                                       AllocationTempData* allocData, HashBlockVisibility* blockVisibilityTypes);

__global__ void setVisibleEntriesToVisibleAtPreviousFrameAndUnstreamed(HashBlockVisibility* blockVisibilityTypes,
                                                                       const int* visibleBlockHashCodes, int noVisibleEntries);

template<bool useSwapping>
__global__ void buildVisibleList_device(ITMHashEntry* hashTable, ITMHashSwapState* swapStates, int hashEntryCount,
                                        int* visibleEntryIDs, AllocationTempData* allocData,
                                        HashBlockVisibility* entriesVisibleType,
                                        Matrix4f M_d, Vector4f projParams_d, Vector2i depthImgSize, float voxelSize);

}

// host methods

template<class TVoxel>
SceneReconstructionEngine_CUDA<TVoxel, VoxelBlockHash>::SceneReconstructionEngine_CUDA(void) {
	ORcudaSafeCall(cudaMalloc((void**) &allocationTempData_device, sizeof(AllocationTempData)));
	ORcudaSafeCall(cudaMallocHost((void**) &allocationTempData_host, sizeof(AllocationTempData)));
}

template<class TVoxel>
SceneReconstructionEngine_CUDA<TVoxel, VoxelBlockHash>::~SceneReconstructionEngine_CUDA(void) {
	ORcudaSafeCall(cudaFreeHost(allocationTempData_host));
	ORcudaSafeCall(cudaFree(allocationTempData_device));
}

template<class TVoxel>
void SceneReconstructionEngine_CUDA<TVoxel, VoxelBlockHash>::ResetScene(
		VoxelVolume<TVoxel, VoxelBlockHash>* scene) {
	int numBlocks = scene->index.GetAllocatedBlockCount();
	int blockSize = scene->index.GetVoxelBlockSize();

	TVoxel* voxelBlocks_ptr = scene->localVBA.GetVoxelBlocks();
	memsetKernel<TVoxel>(voxelBlocks_ptr, TVoxel(), numBlocks * blockSize);
	int* vbaAllocationList_ptr = scene->localVBA.GetAllocationList();
	fillArrayKernel<int>(vbaAllocationList_ptr, numBlocks);
	scene->localVBA.lastFreeBlockId = numBlocks - 1;

	ITMHashEntry tmpEntry;
	memset(&tmpEntry, 0, sizeof(ITMHashEntry));
	tmpEntry.ptr = -2;
	ITMHashEntry* hashEntry_ptr = scene->index.GetEntries();
	memsetKernel<ITMHashEntry>(hashEntry_ptr, tmpEntry, scene->index.hashEntryCount);
	int* excessList_ptr = scene->index.GetExcessAllocationList();
	fillArrayKernel<int>(excessList_ptr, scene->index.excessListSize);

	scene->index.SetLastFreeExcessListId(scene->index.excessListSize - 1);
}

template<class TVoxel>
void SceneReconstructionEngine_CUDA<TVoxel, VoxelBlockHash>::AllocateSceneFromDepth(
		VoxelVolume<TVoxel, VoxelBlockHash>* scene, const ITMView* view,
		const ITMTrackingState* trackingState, const RenderState* renderState, bool onlyUpdateVisibleList,
		bool resetVisibleList) {
	Vector2i depthImgSize = view->depth->noDims;
	float voxelSize = scene->sceneParams->voxel_size;

	Matrix4f M_d, invM_d;
	Vector4f projParams_d, invProjParams_d;

	if (resetVisibleList) scene->index.SetUtilizedHashBlockCount(0);

	M_d = trackingState->pose_d->GetM();
	M_d.inv(invM_d);

	projParams_d = view->calib.intrinsics_d.projectionParamsSimple.all;
	invProjParams_d = projParams_d;
	invProjParams_d.x = 1.0f / invProjParams_d.x;
	invProjParams_d.y = 1.0f / invProjParams_d.y;

	float mu = scene->sceneParams->narrow_band_half_width;

	float* depth = view->depth->GetData(MEMORYDEVICE_CUDA);
	int* voxelAllocationList = scene->localVBA.GetAllocationList();
	int* excessAllocationList = scene->index.GetExcessAllocationList();
	ITMHashEntry* hashTable = scene->index.GetEntries();
	ITMHashSwapState* swapStates = scene->Swapping() ? scene->globalCache->GetSwapStates(true) : 0;

	const int hashEntryCount = scene->index.hashEntryCount;

	int* visibleBlockHashCodes = scene->index.GetUtilizedBlockHashCodes();
	HashBlockVisibility* blockVisibilityTypes = scene->index.GetBlockVisibilityTypes();

	dim3 cudaBlockSizeHV(16, 16);
	dim3 gridSizeHV((int) ceil((float) depthImgSize.x / (float) cudaBlockSizeHV.x),
	                (int) ceil((float) depthImgSize.y / (float) cudaBlockSizeHV.y));

	dim3 cudaBlockSizeAL(256, 1);
	dim3 gridSizeAL((int) ceil((float) hashEntryCount / (float) cudaBlockSizeAL.x));

	dim3 cudaBlockSizeVS(256, 1);
	dim3 gridSizeVS((int) ceil((float) scene->index.GetUtilizedHashBlockCount() / (float) cudaBlockSizeVS.x));

	float oneOverHashBlockSize = 1.0f / (voxelSize * VOXEL_BLOCK_SIZE);
	float surface_cutoff_distance = configuration::get().general_voxel_volume_parameters.block_allocation_band_factor * mu;

	AllocationTempData* tempData = (AllocationTempData*) allocationTempData_host;
	tempData->noAllocatedVoxelEntries = scene->localVBA.lastFreeBlockId;
	tempData->noAllocatedExcessEntries = scene->index.GetLastFreeExcessListId();
	tempData->visibleBlockCount = 0;
	ORcudaSafeCall(
			cudaMemcpyAsync(allocationTempData_device, tempData, sizeof(AllocationTempData), cudaMemcpyHostToDevice));

	ITMLib::HashEntryAllocationState* hashEntryStates_device = scene->index.GetHashEntryAllocationStates();
	scene->index.ClearHashEntryAllocationStates();
	Vector3s* blockCoordinates_device = scene->index.GetAllocationBlockCoordinates();

	if (gridSizeVS.x > 0) {
		setVisibleEntriesToVisibleAtPreviousFrameAndUnstreamed << < gridSizeVS, cudaBlockSizeVS >> >
		                                                                        (blockVisibilityTypes, visibleBlockHashCodes, scene->index.GetUtilizedHashBlockCount());
		ORcudaKernelCheck;
	}

	buildHashAllocAndVisibleType_device << < gridSizeHV, cudaBlockSizeHV >> >
	                                                     (hashEntryStates_device, blockVisibilityTypes,
			                                                     blockCoordinates_device, depth, invM_d, invProjParams_d, surface_cutoff_distance, depthImgSize, oneOverHashBlockSize, hashTable,
			                                                     scene->sceneParams->near_clipping_distance, scene->sceneParams->far_clipping_distance);
	ORcudaKernelCheck;

	bool useSwapping = scene->Swapping();
	if (onlyUpdateVisibleList) useSwapping = false;
	if (!onlyUpdateVisibleList) {
		allocateVoxelBlocksList_device << < gridSizeAL, cudaBlockSizeAL >> > (
				voxelAllocationList, excessAllocationList, hashTable,
						hashEntryCount, (AllocationTempData*) allocationTempData_device,
						hashEntryStates_device, blockVisibilityTypes,
						blockCoordinates_device);
		ORcudaKernelCheck;
	}

	ORcudaSafeCall(cudaFree(hashEntryStates_device));
	ORcudaSafeCall(cudaFree(blockCoordinates_device));

	if (useSwapping) {
		buildVisibleList_device<true> << < gridSizeAL, cudaBlockSizeAL >> >
		                                               (hashTable, swapStates, hashEntryCount, visibleBlockHashCodes,
				                                               (AllocationTempData*) allocationTempData_device, blockVisibilityTypes, M_d, projParams_d, depthImgSize, voxelSize);
		ORcudaKernelCheck;
	} else {
		buildVisibleList_device<false> << < gridSizeAL, cudaBlockSizeAL >> >
		                                                (hashTable, swapStates, hashEntryCount, visibleBlockHashCodes,
				                                                (AllocationTempData*) allocationTempData_device, blockVisibilityTypes, M_d, projParams_d, depthImgSize, voxelSize);
		ORcudaKernelCheck;
	}

	if (useSwapping) {
		reAllocateSwappedOutVoxelBlocks_device << < gridSizeAL, cudaBlockSizeAL >> >
		                                                        (voxelAllocationList, hashTable, hashEntryCount,
				                                                        (AllocationTempData*) allocationTempData_device, blockVisibilityTypes);
		ORcudaKernelCheck;
	}

	ORcudaSafeCall(cudaMemcpy(tempData, allocationTempData_device, sizeof(AllocationTempData), cudaMemcpyDeviceToHost));
	scene->index.SetUtilizedHashBlockCount(tempData->visibleBlockCount);
	scene->localVBA.lastFreeBlockId = tempData->noAllocatedVoxelEntries;
	scene->index.SetLastFreeExcessListId(tempData->noAllocatedExcessEntries);

}

template<class TVoxel>
void SceneReconstructionEngine_CUDA<TVoxel, VoxelBlockHash>::IntegrateIntoScene(
		VoxelVolume<TVoxel, VoxelBlockHash>* scene, const ITMView* view,
		const ITMTrackingState* trackingState, const RenderState* renderState) {
	Vector2i rgbImgSize = view->rgb->noDims;
	Vector2i depthImgSize = view->depth->noDims;
	float voxelSize = scene->sceneParams->voxel_size;

	Matrix4f M_d, M_rgb;
	Vector4f projParams_d, projParams_rgb;

	if (scene->index.GetUtilizedHashBlockCount() == 0) return;

	M_d = trackingState->pose_d->GetM();
	// compute modelview matrix of the RGB camera
	if (TVoxel::hasColorInformation) M_rgb = view->calib.trafo_rgb_to_depth.calib_inv * M_d;

	projParams_d = view->calib.intrinsics_d.projectionParamsSimple.all;
	projParams_rgb = view->calib.intrinsics_rgb.projectionParamsSimple.all;

	float mu = scene->sceneParams->narrow_band_half_width;
	int maxW = scene->sceneParams->max_integration_weight;

	float* depth = view->depth->GetData(MEMORYDEVICE_CUDA);
	float* confidence = view->depthConfidence->GetData(MEMORYDEVICE_CUDA);
	Vector4u* rgb = view->rgb->GetData(MEMORYDEVICE_CUDA);
	TVoxel* localVBA = scene->localVBA.GetVoxelBlocks();
	ITMHashEntry* hashTable = scene->index.GetEntries();

	int* visibleBlockHashCodes = scene->index.GetUtilizedBlockHashCodes();

	dim3 cudaBlockSize(VOXEL_BLOCK_SIZE, VOXEL_BLOCK_SIZE, VOXEL_BLOCK_SIZE);
	dim3 gridSize(scene->index.GetUtilizedHashBlockCount());

	if (scene->sceneParams->stop_integration_at_max_weight) {
		integrateIntoScene_device<TVoxel, true> << < gridSize, cudaBlockSize >> > (localVBA, hashTable, visibleBlockHashCodes,
				rgb, rgbImgSize, depth, confidence, depthImgSize, M_d, M_rgb, projParams_d, projParams_rgb, voxelSize, mu, maxW);
		ORcudaKernelCheck;
	} else {
		integrateIntoScene_device<TVoxel, false> << < gridSize, cudaBlockSize >> >
		                                                        (localVBA, hashTable, visibleBlockHashCodes,
				                                                        rgb, rgbImgSize, depth, confidence, depthImgSize, M_d, M_rgb, projParams_d, projParams_rgb, voxelSize, mu, maxW);
		ORcudaKernelCheck;
	}
}

// plain voxel array

template<class TVoxel>
void SceneReconstructionEngine_CUDA<TVoxel, PlainVoxelArray>::ResetScene(
		VoxelVolume<TVoxel, PlainVoxelArray>* scene) {
	int numBlocks = scene->index.GetAllocatedBlockCount();
	int blockSize = scene->index.GetVoxelBlockSize();

	TVoxel* voxelBlocks_ptr = scene->localVBA.GetVoxelBlocks();
	memsetKernel<TVoxel>(voxelBlocks_ptr, TVoxel(), numBlocks * blockSize);
	int* vbaAllocationList_ptr = scene->localVBA.GetAllocationList();
	fillArrayKernel<int>(vbaAllocationList_ptr, numBlocks);
	scene->localVBA.lastFreeBlockId = numBlocks - 1;
}

template<class TVoxel>
void SceneReconstructionEngine_CUDA<TVoxel, PlainVoxelArray>::AllocateSceneFromDepth(
		VoxelVolume<TVoxel, PlainVoxelArray>* scene, const ITMView* view,
		const ITMTrackingState* trackingState, const RenderState* renderState, bool onlyUpdateVisibleList,
		bool resetVisibleList) {
}

template<class TVoxel>
void SceneReconstructionEngine_CUDA<TVoxel, PlainVoxelArray>::IntegrateIntoScene(
		VoxelVolume<TVoxel, PlainVoxelArray>* scene, const ITMView* view,
		const ITMTrackingState* trackingState, const RenderState* renderState) {
	Vector2i rgbImgSize = view->rgb->noDims;
	Vector2i depthImgSize = view->depth->noDims;
	float voxelSize = scene->sceneParams->voxel_size;

	Matrix4f M_d, M_rgb;
	Vector4f projParams_d, projParams_rgb;

	M_d = trackingState->pose_d->GetM();
	if (TVoxel::hasColorInformation) M_rgb = view->calib.trafo_rgb_to_depth.calib_inv * M_d;

	projParams_d = view->calib.intrinsics_d.projectionParamsSimple.all;
	projParams_rgb = view->calib.intrinsics_rgb.projectionParamsSimple.all;

	float mu = scene->sceneParams->narrow_band_half_width;
	int maxW = scene->sceneParams->max_integration_weight;

	float* depth = view->depth->GetData(MEMORYDEVICE_CUDA);
	float* confidence = view->depthConfidence->GetData(MEMORYDEVICE_CUDA);
	Vector4u* rgb = view->rgb->GetData(MEMORYDEVICE_CUDA);
	TVoxel* localVBA = scene->localVBA.GetVoxelBlocks();
	const PlainVoxelArray::GridAlignedBox* arrayInfo = scene->index.GetIndexData();

	dim3 cudaBlockSize(8, 8, 8);
	dim3 gridSize(scene->index.GetVolumeSize().x / cudaBlockSize.x, scene->index.GetVolumeSize().y / cudaBlockSize.y,
	              scene->index.GetVolumeSize().z / cudaBlockSize.z);

	if (scene->sceneParams->stop_integration_at_max_weight) {
		integrateIntoScene_device<TVoxel, true> << < gridSize, cudaBlockSize >> > (localVBA, arrayInfo,
				rgb, rgbImgSize, depth, confidence, depthImgSize, M_d, M_rgb, projParams_d, projParams_rgb, voxelSize, mu, maxW);
		ORcudaKernelCheck;
	} else {
		integrateIntoScene_device<TVoxel, false> << < gridSize, cudaBlockSize >> > (localVBA, arrayInfo,
				rgb, rgbImgSize, depth, confidence, depthImgSize, M_d, M_rgb, projParams_d, projParams_rgb, voxelSize, mu, maxW);
		ORcudaKernelCheck;
	}
}

namespace {

// device functions

template<class TVoxel, bool stopMaxW>
__global__ void integrateIntoScene_device(TVoxel* voxelArray, const PlainVoxelArray::GridAlignedBox* arrayInfo,
                                          const Vector4u* rgb, Vector2i rgbImgSize, const float* depth,
                                          const float* confidence,
                                          Vector2i depthImgSize, Matrix4f M_d, Matrix4f M_rgb, Vector4f projParams_d,
                                          Vector4f projParams_rgb, float _voxelSize, float mu, int maxW) {
	int x = blockIdx.x * blockDim.x + threadIdx.x;
	int y = blockIdx.y * blockDim.y + threadIdx.y;
	int z = blockIdx.z * blockDim.z + threadIdx.z;

	Vector4f pt_model;
	int locId;

	locId = x + y * arrayInfo->size.x + z * arrayInfo->size.x * arrayInfo->size.y;

	if (stopMaxW) if (voxelArray[locId].w_depth == maxW) return;
//	if (approximateIntegration) if (voxelArray[locId].w_depth != 0) return;

	pt_model.x = (float) (x + arrayInfo->offset.x) * _voxelSize;
	pt_model.y = (float) (y + arrayInfo->offset.y) * _voxelSize;
	pt_model.z = (float) (z + arrayInfo->offset.z) * _voxelSize;
	pt_model.w = 1.0f;

	ComputeUpdatedVoxelInfo<TVoxel::hasColorInformation, TVoxel::hasConfidenceInformation, TVoxel::hasSemanticInformation, TVoxel>::compute(
			voxelArray[locId], pt_model, M_d, projParams_d, M_rgb, projParams_rgb, mu, maxW, depth, confidence,
			depthImgSize, rgb, rgbImgSize);
}

template<class TVoxel, bool stopMaxW>
__global__ void integrateIntoScene_device(TVoxel* localVBA, const ITMHashEntry* hashTable, int* visibleEntryIDs,
                                          const Vector4u* rgb, Vector2i rgbImgSize, const float* depth,
                                          const float* confidence, Vector2i depthImgSize, Matrix4f M_d, Matrix4f M_rgb,
                                          Vector4f projParams_d,
                                          Vector4f projParams_rgb, float _voxelSize, float mu, int maxW) {
	Vector3i globalPos;
	int entryId = visibleEntryIDs[blockIdx.x];

	const ITMHashEntry& currentHashEntry = hashTable[entryId];

	if (currentHashEntry.ptr < 0) return;

	globalPos = currentHashEntry.pos.toInt() * VOXEL_BLOCK_SIZE;

	TVoxel* localVoxelBlock = &(localVBA[currentHashEntry.ptr * VOXEL_BLOCK_SIZE3]);

	int x = threadIdx.x, y = threadIdx.y, z = threadIdx.z;

	Vector4f pt_model;
	int locId;

	locId = x + y * VOXEL_BLOCK_SIZE + z * VOXEL_BLOCK_SIZE * VOXEL_BLOCK_SIZE;

	pt_model.x = (float) (globalPos.x + x) * _voxelSize;
	pt_model.y = (float) (globalPos.y + y) * _voxelSize;
	pt_model.z = (float) (globalPos.z + z) * _voxelSize;
	pt_model.w = 1.0f;

	ComputeUpdatedVoxelInfo<TVoxel::hasColorInformation, TVoxel::hasConfidenceInformation, TVoxel::hasSemanticInformation, TVoxel>::compute(
			localVoxelBlock[locId],
			pt_model, M_d, projParams_d, M_rgb, projParams_rgb, mu, maxW, depth, confidence, depthImgSize, rgb,
			rgbImgSize);
}

__global__ void buildHashAllocAndVisibleType_device(
		ITMLib::HashEntryAllocationState* hashEntryStates, HashBlockVisibility* blockVisibilityTypes, Vector3s* blockCoords,
		const float* depth,
		Matrix4f invM_d, Vector4f projParams_d, float surface_cutoff_distance, Vector2i _imgSize, float oneOverHashBlockSize_Meters,
		ITMHashEntry* hashTable,
		float near_clipping_distance, float far_clipping_distance) {
	int x = threadIdx.x + blockIdx.x * blockDim.x, y = threadIdx.y + blockIdx.y * blockDim.y;

	if (x > _imgSize.x - 1 || y > _imgSize.y - 1) return;

	bool collisionDetected = false;
	buildHashAllocAndVisibleTypePP(hashEntryStates, blockVisibilityTypes, x, y, blockCoords, depth, invM_d, projParams_d,
	                               surface_cutoff_distance, _imgSize, oneOverHashBlockSize_Meters,
	                               hashTable, near_clipping_distance, far_clipping_distance, collisionDetected);
}

__global__ void setVisibleEntriesToVisibleAtPreviousFrameAndUnstreamed(HashBlockVisibility* blockVisibilityTypes,
                                                                       const int* visibleEntryIDs, int visibleEntryCount) {
	int entryId = threadIdx.x + blockIdx.x * blockDim.x;
	if (entryId >= visibleEntryCount) return;
	blockVisibilityTypes[visibleEntryIDs[entryId]] = VISIBLE_AT_PREVIOUS_FRAME_AND_UNSTREAMED;
}

__global__ void allocateVoxelBlocksList_device(
		int* voxelAllocationList, int* excessAllocationList,
		ITMHashEntry* hashTable, const int hashEntryCount,
		AllocationTempData* allocData, HashEntryAllocationState* hashEntryStates,
		HashBlockVisibility* blockVisibilityTypes,
		Vector3s* blockCoordinates) {
	int targetIdx = threadIdx.x + blockIdx.x * blockDim.x;
	if (targetIdx > hashEntryCount - 1) return;

	int vbaIdx, exlIdx;

	switch (hashEntryStates[targetIdx]) {
		case ITMLib::NEEDS_ALLOCATION_IN_ORDERED_LIST: //needs allocation, fits in the ordered list
			vbaIdx = atomicSub(&allocData->noAllocatedVoxelEntries, 1);

			if (vbaIdx >= 0) //there is room in the voxel block array
			{
				ITMHashEntry hashEntry;
				hashEntry.pos = blockCoordinates[targetIdx];
				hashEntry.ptr = voxelAllocationList[vbaIdx];
				hashEntry.offset = 0;

				hashTable[targetIdx] = hashEntry;
			} else {
				// Mark entry as not visible since we couldn't allocate it but buildHashAllocAndVisibleTypePP changed its state.
				blockVisibilityTypes[targetIdx] = INVISIBLE;

				// Restore the previous value to avoid leaks.
				atomicAdd(&allocData->noAllocatedVoxelEntries, 1);
			}
			break;

		case ITMLib::NEEDS_ALLOCATION_IN_EXCESS_LIST: //needs allocation in the excess list
			vbaIdx = atomicSub(&allocData->noAllocatedVoxelEntries, 1);
			exlIdx = atomicSub(&allocData->noAllocatedExcessEntries, 1);

			if (vbaIdx >= 0 && exlIdx >= 0) //there is room in the voxel block array and excess list
			{
				ITMHashEntry hashEntry;
				hashEntry.pos = blockCoordinates[targetIdx];
				hashEntry.ptr = voxelAllocationList[vbaIdx];
				hashEntry.offset = 0;

				int exlOffset = excessAllocationList[exlIdx];

				hashTable[targetIdx].offset = exlOffset + 1; //connect to child

				hashTable[ORDERED_LIST_SIZE + exlOffset] = hashEntry; //add child to the excess list

				blockVisibilityTypes[ORDERED_LIST_SIZE + exlOffset] = IN_MEMORY_AND_VISIBLE; //make child visible
			} else {
				// No need to mark the entry as not visible since buildHashAllocAndVisibleTypePP did not mark it.
				// Restore the previous values to avoid leaks.
				atomicAdd(&allocData->noAllocatedVoxelEntries, 1);
				atomicAdd(&allocData->noAllocatedExcessEntries, 1);
			}

			break;
	}
}

__global__ void
reAllocateSwappedOutVoxelBlocks_device(int* voxelAllocationList, ITMHashEntry* hashTable, int hashEntryCount,
                                       AllocationTempData* allocData, /*int *countOfAllocatedOrderedEntries,*/
                                       HashBlockVisibility* blockVisibilityTypes) {
	int targetIdx = threadIdx.x + blockIdx.x * blockDim.x;
	if (targetIdx >= hashEntryCount) return;

	int vbaIdx;
	int hashEntry_ptr = hashTable[targetIdx].ptr;

	if (blockVisibilityTypes[targetIdx] > 0 &&
	    hashEntry_ptr == -1) //it is visible and has been previously allocated inside the hash, but deallocated from VBA
	{
		vbaIdx = atomicSub(&allocData->noAllocatedVoxelEntries, 1);
		if (vbaIdx >= 0) hashTable[targetIdx].ptr = voxelAllocationList[vbaIdx];
		else atomicAdd(&allocData->noAllocatedVoxelEntries, 1);
	}
}

template<bool useSwapping>
__global__ void buildVisibleList_device(ITMHashEntry* hashTable, ITMHashSwapState* swapStates, int hashEntryCount,
                                        int* visibleEntryIDs, AllocationTempData* allocData,
                                        HashBlockVisibility* entriesVisibleType, Matrix4f M_d, Vector4f projParams_d,
                                        Vector2i depthImgSize, float voxelSize) {
	int hashCode = threadIdx.x + blockIdx.x * blockDim.x;
	if (hashCode >= hashEntryCount) return;

	__shared__ bool shouldPrefix;
	shouldPrefix = false;
	__syncthreads();

	HashBlockVisibility hashVisibleType = entriesVisibleType[hashCode];
	const ITMHashEntry& hashEntry = hashTable[hashCode];

	if (hashVisibleType == 3) {
		bool isVisibleEnlarged, isVisible;

		if (useSwapping) {
			checkBlockVisibility<true>(isVisible, isVisibleEnlarged, hashEntry.pos, M_d, projParams_d, voxelSize,
			                           depthImgSize);
			if (!isVisibleEnlarged) hashVisibleType = INVISIBLE;
		} else {
			checkBlockVisibility<false>(isVisible, isVisibleEnlarged, hashEntry.pos, M_d, projParams_d, voxelSize,
			                            depthImgSize);
			if (!isVisible) hashVisibleType = INVISIBLE;
		}
		entriesVisibleType[hashCode] = hashVisibleType;
	}

	if (hashVisibleType > 0) shouldPrefix = true;

	if (useSwapping) {
		if (hashVisibleType > 0 && swapStates[hashCode].state != 2) swapStates[hashCode].state = 1;
	}

	__syncthreads();

	if (shouldPrefix) {
		int offset = computePrefixSum_device<int>(hashVisibleType > 0, &allocData->visibleBlockCount,
		                                          blockDim.x * blockDim.y, threadIdx.x);
		if (offset != -1) visibleEntryIDs[offset] = hashCode;
	}

#if 0
	// "active list": blocks that have new information from depth image
	// currently not used...
	__syncthreads();

	if (shouldPrefix)
	{
		int offset = computePrefixSum_device<int>(hashVisibleType == 1, noActiveEntries, blockDim.x * blockDim.y, threadIdx.x);
		if (offset != -1) activeEntryIDs[offset] = hashCode;
	}
#endif
}

}
